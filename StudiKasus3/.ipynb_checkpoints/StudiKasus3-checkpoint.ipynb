{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "aG18fEK8ycNP"
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "FRyOZW44xqIb"
   },
   "outputs": [],
   "source": [
    "def haar(img):\n",
    "    status = False\n",
    "    face_roi = []\n",
    "\n",
    "    # Load Haar Cascade classifier for face detection\n",
    "    face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "\n",
    "    # Convert the image to grayscale (required for face detection)\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Detect faces in the image using the face_cascade\n",
    "    faces = face_cascade.detectMultiScale(gray, scaleFactor=1.5, minNeighbors=5)\n",
    "\n",
    "    # Draw bounding boxes around the detected faces and display the image\n",
    "    for (x, y, w, h) in faces:\n",
    "        # Draw a rectangle around the detected face\n",
    "        face_roi = img[y:y+h, x:x+w]\n",
    "        status = True\n",
    "    return status,face_roi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "iCa5h2g6yNed"
   },
   "outputs": [],
   "source": [
    "#from google.colab import drive\n",
    "#drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "rKx5YZhlx70B"
   },
   "outputs": [],
   "source": [
    "#menentukan direktori/folder data citra yang akan dibuka\n",
    "dirname = '../dataset_CV/'\n",
    "\n",
    "#menentukan ukuran tinggi dan lebar gambar\n",
    "height = 225\n",
    "width = 225\n",
    "dim = (width, height)\n",
    "\n",
    "#mengumpulkan data citra yang akan dibuka dalam satu array\n",
    "tampungan_data= []\n",
    "tampungan_label=[]\n",
    "for path, subdirs, files in os.walk(dirname):\n",
    "    print(path)\n",
    "    for name in files:\n",
    "        img_path = (os.path.join(path, name))\n",
    "\n",
    "        #baca path data\n",
    "        if (img_path.endswith(\"jpg\")): #dengan file berekstensi jpg\n",
    "            img = cv2.imread(img_path) #baca gambar\n",
    "\n",
    "            path_parts = path.split('/')\n",
    "            # Mengambil elemen terakhir dari path_parts sebagai kata terakhir\n",
    "            last_word = path_parts[-1]\n",
    "\n",
    "            #preprocessing data / segentasi  boleh dilakukan disini\n",
    "            status, gambar_haar = haar(img)\n",
    "            if(status):\n",
    "                resized=cv2.resize(gambar_haar,dim, interpolation=cv2.INTER_LINEAR) #resize\n",
    "                tampungan_data.append(resized/255.0) #menumpuk gambar blur pada array tampungan dan di sampling\n",
    "                tampungan_label.append(last_word)\n",
    "    X = np.array(tampungan_data)\n",
    "    y = np.array(tampungan_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "V625T9Ety4Pv"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'y' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mseaborn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01msns\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m list_label\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39munique(\u001b[43my\u001b[49m) \u001b[38;5;66;03m#mendapatkan label unik\u001b[39;00m\n\u001b[0;32m      5\u001b[0m label_dict \u001b[38;5;241m=\u001b[39m {label: idx \u001b[38;5;28;01mfor\u001b[39;00m idx, label \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(list_label)} \u001b[38;5;66;03m#masukkan dalam list\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(label_dict)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'y' is not defined"
     ]
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "list_label=np.unique(y) #mendapatkan label unik\n",
    "label_dict = {label: idx for idx, label in enumerate(list_label)} #masukkan dalam list\n",
    "print(label_dict)\n",
    "label_numerik = [label_dict[s] for s in y] #ubah kedalam numerik\n",
    "label_numerik_array = np.array(label_numerik)\n",
    "\n",
    "# Visualisasikan jumlah dalam plot\n",
    "sns.countplot(x=label_numerik_array)\n",
    "plt.xlabel('Numeric Labels')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Count Plot for Numeric Labels')\n",
    "plt.show()\n",
    "\n",
    "# simpan dalam file npy untuk labeling\n",
    "np.save('../weight/label_knn.npy', label_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4F6ihs_e0DJs"
   },
   "outputs": [],
   "source": [
    "# Randomly select 6 indices from the data\n",
    "random_indices = np.random.choice(len(X), 6, replace=False)\n",
    "\n",
    "# Plot the images\n",
    "plt.figure(figsize=(12, 8))\n",
    "for i, idx in enumerate(random_indices):\n",
    "    plt.subplot(2, 3, i+1)\n",
    "    plt.imshow(X[idx])\n",
    "    plt.title(\"Label: \" + str(y[idx]))\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bZKmihE10L4g"
   },
   "outputs": [],
   "source": [
    "print(f\"awal {X.shape}\")\n",
    "\n",
    "jml_data = X.shape[0]\n",
    "h = X.shape[1]\n",
    "w = X.shape[2]\n",
    "d = X.shape[3]\n",
    "flatten  = h*w*d\n",
    "#untuk shape ML itu 1 dimensi jadi X 3 dimensi harus di reshape jadi 1dimensi\n",
    "X_1d = X.reshape(jml_data, flatten)\n",
    "\n",
    "print(f\"akhir {X_1d.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4rtKX4Re0Qlh"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split #library untuk train test split\n",
    "\n",
    "#melakukan splitting data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_1d, label_numerik_array,test_size=0.20, stratify=y)\n",
    "#train size adalah persentase data test yang di-split dengan proporsi label yang sama\n",
    "\n",
    "print(\"X_train: \"+str(X_train.shape))\n",
    "print(\"X_test: \"+str(X_test.shape))\n",
    "print(\"y_train: \"+str(y_train.shape))\n",
    "print(\"y_test: \"+str(y_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NpRbIfio0WKU"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV,StratifiedKFold #melakukan validasi dengan hasil skor akurasi dengan cross validation\n",
    "\n",
    "parameters = {'n_neighbors':[1, 3, 5, 7],\n",
    "             'metric': ['minkowski','euclidean','manhattan']} #masukan parameter yang akan dilakukan\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "model = KNeighborsClassifier()\n",
    "# Create the StratifiedKFold cross-validation method\n",
    "stratified_kfold = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)\n",
    "\n",
    "\n",
    "clf = GridSearchCV(model, parameters, verbose=3, cv=stratified_kfold, scoring='accuracy') #panggil gridsearch\n",
    "clf.fit(X_train,y_train) #train data\n",
    "model_best = clf.best_estimator_ #model terbaik\n",
    "print(clf.best_estimator_) #model terbaik\n",
    "print(clf.best_score_) #akurasi terbaik"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FMoi53V20fVo"
   },
   "outputs": [],
   "source": [
    "# Visualize the metric distances for each label separately with different marker shapes\n",
    "plt.figure(figsize=(6, 4))\n",
    "# Dictionary to map label to marker shape\n",
    "marker_dict = {0: 's', 1: '^', 2: 'o'}\n",
    "# Get the distances to the k nearest neighbors for each data point\n",
    "distances, _ = model_best.kneighbors(X_train)\n",
    "for label in np.unique(y_train):\n",
    "    # Get the indices of data points belonging to the current label\n",
    "    label_indices = np.where(y_train == label)[0]\n",
    "\n",
    "    # Get the distances to the k nearest neighbors for data points of the current label\n",
    "    label_distances = np.mean(distances[label_indices], axis=1)\n",
    "\n",
    "    # Plot the distances for the current label with the corresponding marker shape\n",
    "    plt.scatter(X_train[label_indices, 0], X_train[label_indices, 1], c=label_distances, cmap='plasma', edgecolors='k', label=f\"Class {label}\", marker=marker_dict[label])\n",
    "\n",
    "plt.colorbar(label='Average Distance to k Nearest Neighbors')\n",
    "plt.xlabel(\"Feature 1\")\n",
    "plt.ylabel(\"Feature 2\")\n",
    "plt.title(\"KNN - Metric Distance Visualization for Each Label\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iawesrfl3A2H"
   },
   "outputs": [],
   "source": [
    "y_pred = model_best.predict(X_test) #predict untuk memprediksi data test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5qMWGbtL3E3t"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report , confusion_matrix\n",
    "\n",
    "# Create a confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "# Plot the confusion matrix\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "plt.xlabel(\"Predicted Labels\")\n",
    "plt.ylabel(\"True Labels\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "djZ9q8qS3suA"
   },
   "outputs": [],
   "source": [
    "print(classification_report(y_test, y_pred)) #evaluasi hasil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XFVBvo1M35fL"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import os\n",
    "import os.path\n",
    "import numpy as np\n",
    "\n",
    "url= '../dataset_CV/mahfud/mahfud7.jpg'\n",
    "img=cv2.imread(url)\n",
    "plt.figure()\n",
    "plt.title(\"Data Test\")\n",
    "plt.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n",
    "plt.show()\n",
    "\n",
    "#pastikan langkah preprocessing yang dilakukan sama dengan data train\n",
    "status,haarnya=haar(img)\n",
    "convert = haarnya/255.0\n",
    "img_resize = cv2.resize(convert,(225,225))\n",
    "#tampilkan hasil\n",
    "plt.figure()\n",
    "plt.title(\"Hasil preprocessing\")\n",
    "plt.imshow(img_resize)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "test=[img_resize.flatten()] #makukan ke list\n",
    "print(f\"ukuran gambar test {img_resize.flatten().shape}\") #sama dengan input shape\n",
    "\n",
    "# Mengecek hasil klasifikasi pada salah satu dataset\n",
    "probability=model_best.predict_proba(test)\n",
    "print(f\"nilai probabilitas {probability}\") #tampilkan nilai probabilitas tiap kelas\n",
    "\n",
    "\n",
    "\n",
    "for ind,val in enumerate(label_dict): #mendapatkan nama kelas dan hasil akurasi\n",
    "    print(f'{val} = {probability[0][ind]*100}%')\n",
    "\n",
    "\n",
    "hasil = np.argmax(probability, axis=-1) #mendapatkan kelas dari probabilitas terbaik\n",
    "key_found = [key for key, value in label_dict.items() if value == hasil] #dapatkan namanya\n",
    "print(f\"prediksinya: {key_found}\")\n",
    "print(f\"The predicted image is : {str(hasil)} -> {key_found}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b2tau47R4GN5"
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle.dump(model_best, open('..\\Materi Kuliah Smt 7\\Bengkel Koding\\StudiKasus3\\weight/model_haar_knn_optimasi.pkl', 'wb')) #simpan dalam file.pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Aoo3oTjE4WGI"
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "import os.path\n",
    "import numpy as np\n",
    "from time import sleep\n",
    "import pickle\n",
    "\n",
    "def read_model(filename, path=\"\"):\n",
    "    with open(os.path.join(path, filename), 'rb') as in_name:\n",
    "        model = pickle.load(in_name)\n",
    "        return model\n",
    "\n",
    "\n",
    "color = (255, 0, 0)\n",
    "cap = cv2.VideoCapture(0)\n",
    "model = read_model(\"..\\Materi Kuliah Smt 7\\Bengkel Koding\\StudiKasus3\\weight\\model_haar_knn_optimasi.pkl\", path=\"\") #load model\n",
    "label_dict = np.load('..\\Materi Kuliah Smt 7\\Bengkel Koding\\StudiKasus3\\weight\\label_knn.npy', allow_pickle=True).item() #load label\n",
    "\n",
    "\n",
    "while (cap.isOpened()):\n",
    "    ret , frame = cap.read() #baca vidio dengan looping gambar\n",
    "    if ret:\n",
    "        face_roi = []# Load Haar Cascade classifier for face detection\n",
    "        face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "        # Convert the image to grayscale (required for face detection)\n",
    "        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "        # Detect faces in the image using the face_cascade\n",
    "        faces = face_cascade.detectMultiScale(gray, scaleFactor=1.5, minNeighbors=5)\n",
    "        # Draw bounding boxes around the detected faces and display the image\n",
    "        for (x, y, w, h) in faces:\n",
    "            # Draw a rectangle around the detected face\n",
    "            face_roi = frame[y:y+h, x:x+w]\n",
    "            cv2.rectangle(frame, (x-5, y-5), (x + w+5, y + h+5), (0, 255, 0), 4)#beri rectangle dan beri overlap sebesar 5\n",
    "\n",
    "            convert = face_roi/255.0 #preprocessing\n",
    "            muka = cv2.resize(convert, (225,225), interpolation = cv2.INTER_AREA)#wajib sama dengan citra inputan trainer\n",
    "            cv2.imshow(\"Detect\",muka)\n",
    "            gambar_flat=[muka.flatten()] #jadikan 1 dimensi\n",
    "            prediksi= model.predict(gambar_flat) #prediksi\n",
    "            key_found = [key for key, value in label_dict.items() if value == prediksi] #dapatkan namanya\n",
    "            cv2.putText(frame, f\"Deteksi : {key_found[0]}\", (20, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255),2)\n",
    "            cv2.imshow(\"Detect\",muka)\n",
    "        cv2.imshow(\"Video Original\" , frame)\n",
    "    else:\n",
    "        print('no video')\n",
    "        cap.set(cv2.CAP_PROP_POS_FRAMES, 0)\n",
    "        continue\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "cv2.waitKey(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "beKXQ38XUW8D"
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "def haar(img):\n",
    "    face_roi = []\n",
    "    status = False\n",
    "\n",
    "    # Load Haar Cascade classifier for face detection\n",
    "    face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "\n",
    "    # Convert the image to grayscale (required for face detection)\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Detect faces in the image using the face_cascade\n",
    "    faces = face_cascade.detectMultiScale(gray, scaleFactor=1.5, minNeighbors=5)\n",
    "\n",
    "    # Draw bounding boxes around the detected faces and display the image\n",
    "    for (x, y, w, h) in faces:\n",
    "        # Draw a rectangle around the detected face\n",
    "        face_roi = img[y:y+h, x:x+w]\n",
    "        status = True\n",
    "    return status,face_roi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Y3ON9DEAUbSg"
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Loading the image\n",
    "img = cv2.imread('../dataset_CV/mahfud/mahfud2.jpg')\n",
    "status,haarnya=haar(img)\n",
    "\n",
    "# Applying SIFT detector\n",
    "sift = cv2.SIFT_create(500)\n",
    "kpts, des = sift.detectAndCompute(haarnya, None)\n",
    "\n",
    "# Marking the keypoint on the image using circles\n",
    "img=cv2.drawKeypoints(haarnya, kpts , haarnya ,flags=cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS)\n",
    "print(f\" jumlah keypoint terbentuk {len(kpts)}\")\n",
    "plt.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YVbM-vgGUsnP"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "#kmeans works only on float, so convert integers to float\n",
    "descriptors_float = des.astype(float)\n",
    "\n",
    "# Perform k-means clustering and vector quantization\n",
    "\n",
    "from scipy.cluster.vq import kmeans, vq\n",
    "k = 48  #dari total 500 diambil hanya 200\n",
    "voc, variance = kmeans(obs=descriptors_float, k_or_guess=k, iter=5)\n",
    "\n",
    "\n",
    "# Calculate the histogram of features and represent them as vector\n",
    "#vq Assigns codes from a code book to observations.\n",
    "im_features = np.zeros((1, k), \"float32\")\n",
    "for i in range(1):\n",
    "    words, distance = vq(des,voc)\n",
    "    for w in words:\n",
    "        im_features[i][w] += 1\n",
    "print(im_features.shape)\n",
    "print(im_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DDRWl4ScU0Fd"
   },
   "outputs": [],
   "source": [
    "#menentukan direktori/folder data citra yang akan dibuka\n",
    "dirname = '../dataset_CV/'\n",
    "\n",
    "#menentukan ukuran tinggi dan lebar gambar\n",
    "height = 225\n",
    "width = 225\n",
    "dim = (width, height)\n",
    "\n",
    "#BRISK is a good replacement to SIFT. ORB also works but didn;t work well for this example\n",
    "sift = cv2.SIFT_create()\n",
    "#mengumpulkan data citra yang akan dibuka dalam satu array\n",
    "tampungan_data = []\n",
    "tampungan_label = []\n",
    "for path, subdirs, files in os.walk(dirname):\n",
    "    print(path)\n",
    "    for name in files:\n",
    "        img_path = (os.path.join(path, name))  #baca path data\n",
    "        if (img_path.endswith(\"jpg\")): #dengan file berekstensi jpg\n",
    "            img = cv2.imread(img_path) #baca gambar\n",
    "            status, haarnya = haar(img)\n",
    "            if(status):\n",
    "                resized=cv2.resize(haarnya,dim, interpolation=cv2.INTER_LINEAR) #resize\n",
    "                kpts, des = sift.detectAndCompute(resized, None)\n",
    "                tampungan_data.append(des)\n",
    "\n",
    "                path_parts = path.split('/')\n",
    "                # Mengambil elemen terakhir dari path_parts sebagai kata terakhir\n",
    "                last_word = path_parts[-1]\n",
    "                #preprocessing data / segentasi  boleh dilakukan disini\n",
    "                tampungan_label.append(last_word)\n",
    "    X = np.array(tampungan_data, dtype=object)\n",
    "    y = np.array(tampungan_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Vm20dGcRU-7D"
   },
   "outputs": [],
   "source": [
    "descriptors = None\n",
    "for descriptor in X:\n",
    "    if descriptors is None:\n",
    "        descriptors = descriptor\n",
    "    else:\n",
    "        descriptors = np.vstack((descriptors, descriptor)) #gunakan untuk menggabungkan deskriptor menjadi satu tumpukan\n",
    "\n",
    "#kmeans works only on float, so convert integers to float\n",
    "descriptors_float = descriptors.astype(float)\n",
    "# Perform k-means clustering and vector quantization\n",
    "from scipy.cluster.vq import kmeans, vq\n",
    "\n",
    "k = 30  #k means with 100 clusters gives lower accuracy for the aeroplane example\n",
    "voc, variance = kmeans(obs=descriptors_float, k_or_guess=k, iter=5)\n",
    "\n",
    "# Calculate the histogram of features and represent them as vector\n",
    "# vq Assigns codes from a code book to observations.\n",
    "im_features = np.zeros((len(y), k), \"float32\")\n",
    "for i in range(len(y)):\n",
    "    words, distance = vq(X[i],voc)\n",
    "    for w in words:\n",
    "        im_features[i][w] += 1\n",
    "print(im_features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Prsdr4kjVE9Q"
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "list_label=np.unique(y) #mendapatkan label unik\n",
    "label_dict = {label: idx for idx, label in enumerate(list_label)} #masukkan dalam list\n",
    "print(f\"{label_dict} jumlah data: {len(y)}\")\n",
    "\n",
    "label_numerik = [label_dict[s] for s in y] #ubah kedalam numerik\n",
    "label_numerik_array = np.array(label_numerik)\n",
    "\n",
    "# Visualisasikan dalam jumlah dalam plot\n",
    "sns.countplot(x=label_numerik_array)\n",
    "plt.xlabel('Numeric Labels')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Count Plot for Numeric Labels')\n",
    "plt.show()\n",
    "\n",
    "# simpan dalam file npy untuk labeling\n",
    "np.save('../Materi Kuliah Smt 7/Bengkel Koding/StudiKasus3//weight/label_knn1.npy', label_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A1v40sxEViYq"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split #library untuk train test split\n",
    "\n",
    "#melakukan splitting data\n",
    "X_train, X_test, y_train, y_test = train_test_split(im_features, label_numerik_array,test_size=0.20, stratify=label_numerik_array)\n",
    "#train size adalah persentase data test yang di-split dengan proporsi label yang sama\n",
    "\n",
    "print(\"X_train: \"+str(X_train.shape))\n",
    "print(\"X_test: \"+str(X_test.shape))\n",
    "print(\"y_train: \"+str(y_train.shape))\n",
    "print(\"y_test: \"+str(y_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xY_L90EuVtI_"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV,StratifiedKFold #melakukan validasi dengan hasil skor akurasi dengan cross validation\n",
    "parameters = {'n_neighbors':[1, 3, 5, 7],\n",
    "             'metric': ['minkowski','euclidean','manhattan']} #masukan parameter yang akan dilakukan\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "model = KNeighborsClassifier()\n",
    "\n",
    "# Create the StratifiedKFold cross-validation method\n",
    "stratified_kfold = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)\n",
    "\n",
    "\n",
    "clf = GridSearchCV(model, parameters, verbose=3, cv=stratified_kfold, scoring='accuracy') #panggil gridsearch\n",
    "clf.fit(X_train,y_train) #train data\n",
    "best = clf.best_estimator_ #model terbaik\n",
    "print(clf.best_estimator_) #model terbaik\n",
    "print(clf.best_score_) #akurasi terbaik"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lZOE6aahVv8i"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Visualize the metric distances for each label separately with different marker shapes\n",
    "plt.figure(figsize=(8, 6))\n",
    "distances, _ = best.kneighbors(X_train)\n",
    "\n",
    "# Dictionary to map label to marker shape\n",
    "marker_dict = {0: 's', 1: '^', 2: 'o'}\n",
    "\n",
    "for label in np.unique(y_train):\n",
    "    # Get the indices of data points belonging to the current label\n",
    "    label_indices = np.where(y_train == label)[0]\n",
    "\n",
    "    # Get the distances to the k nearest neighbors for data points of the current label\n",
    "    label_distances = np.mean(distances[label_indices], axis=1)\n",
    "\n",
    "    # Plot the distances for the current label with the corresponding marker shape\n",
    "    plt.scatter(X_train[label_indices, 0], X_train[label_indices, 1], c=label_distances, cmap='plasma', edgecolors='k', label=f\"Class {label}\", marker=marker_dict[label])\n",
    "\n",
    "plt.colorbar(label='Average Distance to k Nearest Neighbors')\n",
    "plt.xlabel(\"Feature 1\")\n",
    "plt.ylabel(\"Feature 2\")\n",
    "plt.title(\"KNN - Metric Distance Visualization for Each Label\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LzHBMe6mVypE"
   },
   "outputs": [],
   "source": [
    "y_pred = best.predict(X_test) #predict untuk memprediksi data test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DYh5RfwSV15r"
   },
   "outputs": [],
   "source": [
    "# Create a confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "# Plot the confusion matrix\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "plt.xlabel(\"Predicted Labels\")\n",
    "plt.ylabel(\"True Labels\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Gdt6L_8DV4km"
   },
   "outputs": [],
   "source": [
    "print(classification_report(y_test, y_pred)) #evaluasi hasil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gg5Vg1l6V6qN"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import os\n",
    "import os.path\n",
    "import numpy as np\n",
    "\n",
    "url= '../dataset/mahfud/mahfud5.jpg'\n",
    "img=cv2.imread(url)\n",
    "plt.figure()\n",
    "plt.title(\"Data Test\")\n",
    "plt.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n",
    "plt.show()\n",
    "\n",
    "#pastikan langkah preprocessing yang dilakukan sama dengan data train\n",
    "status,haarnya=haar(img)\n",
    "img_resize = cv2.resize(haarnya,(225,225))\n",
    "#tampilkan hasil\n",
    "plt.figure()\n",
    "plt.title(\"Hasil Haar\")\n",
    "plt.imshow(cv2.cvtColor(img_resize, cv2.COLOR_BGR2RGB))\n",
    "plt.show()\n",
    "\n",
    "\n",
    "plt.figure()\n",
    "plt.title(\"Hasil Sift\")\n",
    "# sift = cv2.SIFT_create()\n",
    "sift = cv2.SIFT_create(nfeatures=500, nOctaveLayers=9, contrastThreshold=0.03, edgeThreshold=10, sigma=1.6)\n",
    "kpts, des = sift.detectAndCompute(img_resize, None)\n",
    "# Marking the keypoint on the image using circles\n",
    "img=cv2.drawKeypoints(img_resize, kpts , img_resize ,flags=cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS)\n",
    "print(f\" jumlah keypoint terbentuk {len(kpts)}\")\n",
    "plt.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# bovw\n",
    "#kmeans works only on float, so convert integers to float\n",
    "descriptors_float = des.astype(float)\n",
    "# Perform k-means clustering and vector quantization\n",
    "from scipy.cluster.vq import kmeans, vq\n",
    "k = 30  #dari total 500 diambil hanya 200\n",
    "voc, variance = kmeans(obs=descriptors_float, k_or_guess=k, iter=5)\n",
    "# Calculate the histogram of features and represent them as vector\n",
    "#vq Assigns codes from a code book to observations.\n",
    "im_features = np.zeros((1, k), \"float32\")\n",
    "for i in range(1):\n",
    "    words, distance = vq(des,voc)\n",
    "    for w in words:\n",
    "        im_features[i][w] += 1\n",
    "print(im_features.shape)\n",
    "\n",
    "print(f\"ukuran data test {im_features.shape}\") #sama dengan input shape\n",
    "\n",
    "# Mengecek hasil klasifikasi pada salah satu dataset\n",
    "probability=best.predict_proba(im_features)\n",
    "print(f\"nilai probabilitas {probability}\") #tampilkan nilai probabilitas tiap kelas\n",
    "\n",
    "\n",
    "\n",
    "for ind,val in enumerate(label_dict): #mendapatkan nama kelas dan hasil akurasi\n",
    "    print(f'{val} = {probability[0][ind]*100}%')\n",
    "\n",
    "\n",
    "hasil = np.argmax(probability, axis=-1) #mendapatkan kelas dari probabilitas terbaik\n",
    "key_found = [key for key, value in label_dict.items() if value == hasil] #dapatkan namanya\n",
    "print(f\"prediksinya: {key_found}\")\n",
    "print(f\"The predicted image is : {str(hasil)} -> {key_found}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "p4OOx4ruWVXN"
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle.dump(best, open('../Materi Kuliah Smt 7/Bengkel Koding/StudiKasus3/weight/model_haar_sift_knn_optimasi.pkl', 'wb')) #simpan dalam file.pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "p7CAHMySWg3L"
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "import os.path\n",
    "import numpy as np\n",
    "from time import sleep\n",
    "import pickle\n",
    "\n",
    "def read_model(filename, path=\"\"):\n",
    "    with open(os.path.join(path, filename), 'rb') as in_name:\n",
    "        model = pickle.load(in_name)\n",
    "        return model\n",
    "\n",
    "\n",
    "color = (255, 0, 0)\n",
    "cap = cv2.VideoCapture(0) # 0 jika kamera\n",
    "model = read_model(\"../Materi Kuliah Smt 7/Bengkel Koding/StudiKasus3/weight/model_haar_sift_knn_optimasi.pkl\", path=\"\") #load model\n",
    "label_dict = np.load('../Materi Kuliah Smt 7/Bengkel Koding/StudiKasus3/weight/label_knn1.npy', allow_pickle=True).item() #load label\n",
    "\n",
    "\n",
    "while (cap.isOpened()):\n",
    "    ret , frame = cap.read() #baca vidio dengan looping gambar\n",
    "    if ret:\n",
    "        face_roi = [] # Load Haar Cascade classifier for face detection\n",
    "        face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "        # Convert the image to grayscale (required for face detection)\n",
    "        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "        # Detect faces in the image using the face_cascade\n",
    "        faces = face_cascade.detectMultiScale(gray, scaleFactor=1.5, minNeighbors=5)\n",
    "        # Draw bounding boxes around the detected faces and display the image\n",
    "        for (x, y, w, h) in faces:\n",
    "            # Draw a rectangle around the detected face\n",
    "            face_roi = frame[y:y+h, x:x+w]\n",
    "            cv2.rectangle(frame, (x-5, y-5), (x + w+5, y + h+5), (0, 255, 0), 4)#beri rectangle dan beri overlap sebesar 5\n",
    "            muka = cv2.resize(face_roi, (225,225), interpolation = cv2.INTER_AREA)#wajib sama dengan citra inputan trainer\n",
    "            #sift\n",
    "            sift = cv2.SIFT_create(nfeatures=500, nOctaveLayers=9, contrastThreshold=0.03, edgeThreshold=10, sigma=1.6)\n",
    "            kpts, des = sift.detectAndCompute(muka, None)\n",
    "            print(f\" jumlah keypoint terbentuk {len(kpts)}\")\n",
    "            if(len(kpts) >= 200): #karna sistem akan akan menggunakan shape 200 minimal\n",
    "                cv2.imshow(\"Haar\",muka)\n",
    "                # Marking the keypoint on the image using circles\n",
    "                img=cv2.drawKeypoints(muka, kpts , muka ,flags=cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS)\n",
    "                # bovw\n",
    "                #kmeans works only on float, so convert integers to float\n",
    "                descriptors_float = des.astype(float)\n",
    "                # Perform k-means clustering and vector quantization\n",
    "                from scipy.cluster.vq import kmeans, vq\n",
    "                k = 30  #dari total 500 diambil hanya 200\n",
    "                voc, variance = kmeans(obs=descriptors_float, k_or_guess=k, iter=5)\n",
    "                # Calculate the histogram of features and represent them as vector\n",
    "                #vq Assigns codes from a code book to observations.\n",
    "                im_features = np.zeros((1, k), \"float32\")\n",
    "                for i in range(1):\n",
    "                    words, distance = vq(des,voc)\n",
    "                    for w in words:\n",
    "                        im_features[i][w] += 1\n",
    "\n",
    "                cv2.imshow(\"Sift\",img)\n",
    "                prediksi= model.predict(im_features) #prediksi\n",
    "                key_found = [key for key, value in label_dict.items() if value == prediksi] #dapatkan namanya\n",
    "                cv2.putText(frame, f\"Deteksi : {key_found[0]}\", (20, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255),2)\n",
    "        cv2.imshow(\"Video Original\" , frame)\n",
    "    else:\n",
    "        print('no video')\n",
    "        cap.set(cv2.CAP_PROP_POS_FRAMES, 0)\n",
    "        continue\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "cv2.waitKey(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CCv6QfcAmR_i"
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "import os.path\n",
    "import numpy as np\n",
    "from time import sleep\n",
    "import pickle\n",
    "\n",
    "def read_model(filename, path=\"\"):\n",
    "    with open(os.path.join(path, filename), 'rb') as in_name:\n",
    "        model = pickle.load(in_name)\n",
    "        return model\n",
    "\n",
    "\n",
    "color = (255, 0, 0)\n",
    "video_path = input(\"Masukkan path video: \")  # Meminta pengguna untuk memasukkan path video\n",
    "cap = cv2.VideoCapture(video_path)  # Membaca video dari file yang ditentukan\n",
    "\n",
    "model = read_model(\"../Materi Kuliah Smt 7/Bengkel Koding/StudiKasus3/weight/model_haar_sift_knn_optimasi.pkl\", path=\"\") #load model\n",
    "label_dict = np.load('../Materi Kuliah Smt 7/Bengkel Koding/StudiKasus3/weight/label_knn1.npy', allow_pickle=True).item() #load label\n",
    "\n",
    "\n",
    "while (cap.isOpened()):\n",
    "    ret , frame = cap.read() #baca video dengan looping gambar\n",
    "    if ret:\n",
    "        face_roi = []# Load Haar Cascade classifier for face detection\n",
    "        face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "        # Convert the image to grayscale (required for face detection)\n",
    "        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "        # Detect faces in the image using the face_cascade\n",
    "        faces = face_cascade.detectMultiScale(gray, scaleFactor=1.5, minNeighbors=5)\n",
    "        # Draw bounding boxes around the detected faces and display the image\n",
    "        for (x, y, w, h) in faces:\n",
    "            # Draw a rectangle around the detected face\n",
    "            face_roi = frame[y:y+h, x:x+w]\n",
    "            cv2.rectangle(frame, (x-5, y-5), (x + w+5, y + h+5), (0, 255, 0), 4)#beri rectangle dan beri overlap sebesar 5\n",
    "            muka = cv2.resize(face_roi, (225,225), interpolation = cv2.INTER_AREA)#wajib sama dengan citra inputan trainer\n",
    "            #sift\n",
    "            sift = cv2.SIFT_create(nfeatures=500, nOctaveLayers=9, contrastThreshold=0.03, edgeThreshold=10, sigma=1.6)\n",
    "            kpts, des = sift.detectAndCompute(muka, None)\n",
    "            print(f\" jumlah keypoint terbentuk {len(kpts)}\")\n",
    "            if(len(kpts) >= 200): #karna sistem akan akan menggunakan shape 200 minimal\n",
    "                cv2.imshow(\"Haar\",muka)\n",
    "                # Marking the keypoint on the image using circles\n",
    "                img=cv2.drawKeypoints(muka, kpts , muka ,flags=cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS)\n",
    "                # bovw\n",
    "                #kmeans works only on float, so convert integers to float\n",
    "                descriptors_float = des.astype(float)\n",
    "                # Perform k-means clustering and vector quantization\n",
    "                from scipy.cluster.vq import kmeans, vq\n",
    "                k = 200  #dari total 500 diambil hanya 200\n",
    "                voc, variance = kmeans(obs=descriptors_float, k_or_guess=k, iter=5)\n",
    "                # Calculate the histogram of features and represent them as vector\n",
    "                #vq Assigns codes from a code book to observations.\n",
    "                im_features = np.zeros((1, k), \"float32\")\n",
    "                for i in range(1):\n",
    "                    words, distance = vq(des,voc)\n",
    "                    for w in words:\n",
    "                        im_features[i][w] += 1\n",
    "\n",
    "                cv2.imshow(\"Sift\",img)\n",
    "                prediksi= model.predict(im_features) #prediksi\n",
    "                key_found = [key for key, value in label_dict.items() if value == prediksi] #dapatkan namanya\n",
    "                cv2.putText(frame, f\"Deteksi : {key_found[0]}\", (20, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255),2)\n",
    "        cv2.imshow(\"Video Original\" , frame)\n",
    "    else:\n",
    "        print('no video')\n",
    "        cap.set(cv2.CAP_PROP_POS_FRAMES, 0)\n",
    "        continue\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "cv2.waitKey(1)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "private_outputs": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3.9.1 (tensorflow)",
   "language": "python",
   "name": "latihancv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
